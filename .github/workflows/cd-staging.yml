name: ðŸŽ­ Deploy to Staging

on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      force_deploy:
        description: 'Force deployment (skip checks)'
        type: boolean
        default: false

env:
  REGISTRY: ghcr.io
  IMAGE_BASE: ghcr.io/${{ github.repository_owner }}/vorta

concurrency:
  group: staging-deployment
  cancel-in-progress: true

jobs:
  # Pre-deployment checks
  pre-deployment:
    name: ðŸ” Pre-deployment Checks
    runs-on: ubuntu-latest
    outputs:
      version: ${{ steps.version.outputs.version }}
      should_deploy: ${{ steps.check.outputs.should_deploy }}

    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: ðŸ·ï¸ Generate Version
        id: version
        run: |
          VERSION=$(git describe --tags --always --dirty)-staging-$(date +%Y%m%d-%H%M%S)
          echo "version=$VERSION" >> $GITHUB_OUTPUT
          echo "ðŸ“ Staging Version: $VERSION"

      - name: âœ… Deployment Check
        id: check
        run: |
          SHOULD_DEPLOY="true"

          # Check if CI is passing
          if [[ "${{ github.event.inputs.force_deploy }}" != "true" ]]; then
            # Get latest CI run status
            CI_STATUS=$(gh api repos/${{ github.repository }}/actions/workflows/ci.yml/runs \
              --jq '.workflow_runs[0].conclusion' \
              --method GET)
            
            if [[ "$CI_STATUS" != "success" ]]; then
              echo "âŒ Latest CI run did not succeed: $CI_STATUS"
              SHOULD_DEPLOY="false"
            fi
          fi

          echo "should_deploy=$SHOULD_DEPLOY" >> $GITHUB_OUTPUT
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  # Build and push images
  build-staging-images:
    name: ðŸ³ Build Staging Images
    runs-on: ubuntu-latest
    needs: pre-deployment
    if: needs.pre-deployment.outputs.should_deploy == 'true'

    strategy:
      matrix:
        service: [inference-engine, vector-store, orchestrator, api-gateway]

    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ³ Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: ðŸ”‘ Login to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: ðŸ”¨ Build and Push Staging Image
        uses: docker/build-push-action@v5
        with:
          context: services/${{ matrix.service }}
          push: true
          tags: |
            ${{ env.IMAGE_BASE }}-${{ matrix.service }}:staging
            ${{ env.IMAGE_BASE }}-${{ matrix.service }}:staging-${{ github.sha }}
          platforms: linux/amd64,linux/arm64
          cache-from: type=gha
          cache-to: type=gha,mode=max
          build-args: |
            VERSION=${{ needs.pre-deployment.outputs.version }}
            BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ')
            VCS_REF=${{ github.sha }}
            ENVIRONMENT=staging

  # Security scanning for staging
  staging-security-scan:
    name: ðŸ›¡ï¸ Staging Security Scan
    runs-on: ubuntu-latest
    needs: [pre-deployment, build-staging-images]
    if: needs.pre-deployment.outputs.should_deploy == 'true'

    strategy:
      matrix:
        service: [inference-engine, vector-store, orchestrator, api-gateway]

    steps:
      - name: ðŸ” Run Trivy Scanner
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ env.IMAGE_BASE }}-${{ matrix.service }}:staging-${{ github.sha }}
          format: 'json'
          output: 'trivy-staging-${{ matrix.service }}.json'

      - name: ðŸ“Š Process Security Results
        run: |
          # Check for CRITICAL vulnerabilities
          CRITICAL=$(jq '.Results[]?.Vulnerabilities[]? | select(.Severity == "CRITICAL")' trivy-staging-${{ matrix.service }}.json | wc -l)
          echo "ðŸ” Critical vulnerabilities in ${{ matrix.service }}: $CRITICAL"

          if [[ $CRITICAL -gt 5 ]]; then
            echo "âŒ Too many critical vulnerabilities ($CRITICAL > 5) in ${{ matrix.service }}"
            exit 1
          fi

      - name: ðŸ“„ Upload Security Report
        uses: actions/upload-artifact@v3
        with:
          name: security-scan-staging-${{ matrix.service }}
          path: trivy-staging-${{ matrix.service }}.json

  # Database preparation
  prepare-staging-database:
    name: ðŸ—ƒï¸ Prepare Staging Database
    runs-on: ubuntu-latest
    needs: pre-deployment
    if: needs.pre-deployment.outputs.should_deploy == 'true'

    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: ðŸ“¦ Install Migration Tools
        run: |
          pip install alembic psycopg2-binary

      - name: ðŸ—ƒï¸ Backup Current Database
        run: |
          pg_dump ${{ secrets.STAGING_DATABASE_URL || 'postgresql://localhost:5432/vorta_staging' }} > staging-backup-$(date +%Y%m%d-%H%M%S).sql
        continue-on-error: true

      - name: ðŸ”„ Run Database Migrations
        run: |
          cd services/vector-store
          alembic upgrade head
        env:
          DATABASE_URL: ${{ secrets.STAGING_DATABASE_URL || 'postgresql://localhost:5432/vorta_staging' }}

  # Deploy to staging environment
  deploy-to-staging:
    name: ðŸš€ Deploy to Staging
    runs-on: ubuntu-latest
    needs: [pre-deployment, build-staging-images, staging-security-scan, prepare-staging-database]
    if: needs.pre-deployment.outputs.should_deploy == 'true'
    outputs:
      deployment_url: ${{ steps.deploy.outputs.url }}

    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: âš™ï¸ Setup Kubectl
        uses: azure/setup-kubectl@v3

      - name: â›… Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: '3.13.0'

      - name: ðŸ”‘ Configure Kubernetes Context
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBE_CONFIG_STAGING || '# No kube config available' }}" | base64 -d > ~/.kube/config
          kubectl config current-context

      - name: ðŸ—ï¸ Create Namespace
        run: |
          kubectl create namespace vorta-staging --dry-run=client -o yaml | kubectl apply -f -

      - name: ðŸ” Setup Secrets
        run: |
          # Create staging secrets
          kubectl create secret generic vorta-staging-config \
            --from-literal=database-url="${{ secrets.STAGING_DATABASE_URL || 'postgresql://localhost:5432/vorta_staging' }}" \
            --from-literal=redis-url="${{ secrets.STAGING_REDIS_URL || 'redis://localhost:6379/1' }}" \
            --from-literal=jwt-secret="${{ secrets.STAGING_JWT_SECRET || 'demo-jwt-secret-staging' }}" \
            --from-literal=api-key="${{ secrets.STAGING_API_KEY || 'demo-staging-key' }}" \
            --namespace=vorta-staging \
            --dry-run=client -o yaml | kubectl apply -f -

      - name: ðŸš€ Deploy with Helm
        id: deploy
        run: |
          helm upgrade --install vorta-staging ./infrastructure/helm/vorta \
            --namespace vorta-staging \
            --set global.environment=staging \
            --set global.imageTag=staging-${{ github.sha }} \
            --set global.version="${{ needs.pre-deployment.outputs.version }}" \
            --set global.registry="${{ env.REGISTRY }}" \
            --values infrastructure/helm/values/staging.yaml \
            --wait --timeout=15m

          # Get ingress URL
          STAGING_URL=$(kubectl get ingress vorta-staging-ingress -n vorta-staging -o jsonpath='{.spec.rules[0].host}')
          echo "url=https://$STAGING_URL" >> $GITHUB_OUTPUT
          echo "ðŸŒ Staging URL: https://$STAGING_URL"

      - name: âœ… Verify Deployment
        run: |
          echo "ðŸ” Checking deployment status..."
          kubectl get pods -n vorta-staging

          # Wait for all pods to be ready
          kubectl wait --for=condition=ready pod \
            -l app.kubernetes.io/instance=vorta-staging \
            -n vorta-staging \
            --timeout=600s

          echo "âœ… All pods are ready"

      - name: ðŸ” Health Check
        run: |
          # Wait a bit for services to be fully ready
          sleep 30

          # Health check script
          python scripts/health_check.py \
            --environment=staging \
            --endpoint="${{ steps.deploy.outputs.url }}" \
            --api-key="${{ secrets.STAGING_API_KEY || 'demo-staging-key' }}" \
            --timeout=120

  # Run staging smoke tests
  staging-smoke-tests:
    name: ðŸ§ª Staging Smoke Tests
    runs-on: ubuntu-latest
    needs: [pre-deployment, deploy-to-staging]
    if: needs.pre-deployment.outputs.should_deploy == 'true'

    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: ðŸ“¦ Install Test Dependencies
        run: |
          pip install pytest requests pydantic

      - name: ðŸ§ª Run Smoke Tests
        run: |
          python -m pytest tests/smoke/staging/ -v \
            --tb=short \
            --maxfail=3
        env:
          STAGING_ENDPOINT: ${{ needs.deploy-to-staging.outputs.deployment_url }}
          API_KEY: ${{ secrets.STAGING_API_KEY || 'demo-staging-key' }}
          TEST_TIMEOUT: 30

      - name: ðŸ“„ Upload Test Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: staging-smoke-test-results
          path: tests/smoke/results/

  # Load testing
  staging-load-test:
    name: âš¡ Staging Load Test
    runs-on: ubuntu-latest
    needs: [staging-smoke-tests]
    if: success()

    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: ðŸ Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: ðŸ“¦ Install Load Test Tools
        run: |
          pip install locust requests

      - name: âš¡ Run Load Test
        run: |
          cd tests/performance
          locust -f staging_load_test.py \
            --host=${{ needs.deploy-to-staging.outputs.deployment_url }} \
            --users=50 \
            --spawn-rate=5 \
            --run-time=300s \
            --headless \
            --csv=staging-load-test
        env:
          API_KEY: ${{ secrets.STAGING_API_KEY || 'demo-staging-key' }}

      - name: ðŸ“Š Process Load Test Results
        run: |
          python scripts/analyze_load_test.py tests/performance/staging-load-test_stats.csv

      - name: ðŸ“„ Upload Load Test Results
        uses: actions/upload-artifact@v3
        with:
          name: staging-load-test-results
          path: tests/performance/staging-load-test*

  # Rollback mechanism
  rollback-staging:
    name: â†©ï¸ Rollback Staging
    runs-on: ubuntu-latest
    if: failure() && github.event.inputs.force_deploy != 'true'
    needs: [deploy-to-staging, staging-smoke-tests]
    steps:
      - name: âš™ï¸ Setup Kubectl
        uses: azure/setup-kubectl@v3

      - name: â›… Setup Helm
        uses: azure/setup-helm@v3

      - name: ðŸ”‘ Configure Kubernetes Context
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBE_CONFIG_STAGING || '# No kube config available' }}" | base64 -d > ~/.kube/config

      - name: â†©ï¸ Perform Rollback
        run: |
          echo "ðŸ”„ Rolling back staging deployment..."
          helm rollback vorta-staging -n vorta-staging

          # Wait for rollback to complete
          kubectl wait --for=condition=ready pod \
            -l app.kubernetes.io/instance=vorta-staging \
            -n vorta-staging \
            --timeout=600s

          echo "âœ… Rollback completed"

  # Monitoring and alerting setup
  setup-monitoring:
    name: ðŸ“Š Setup Monitoring
    runs-on: ubuntu-latest
    needs: [deploy-to-staging]
    if: success()

    steps:
      - name: ðŸ“¥ Checkout Code
        uses: actions/checkout@v4

      - name: âš™ï¸ Setup Kubectl
        uses: azure/setup-kubectl@v3

      - name: ðŸ”‘ Configure Kubernetes Context
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBE_CONFIG_STAGING || '# No kube config available' }}" | base64 -d > ~/.kube/config

      - name: ðŸ“Š Deploy Monitoring Stack
        run: |
          # Deploy Prometheus monitoring rules
          kubectl apply -f infrastructure/kubernetes/monitoring/staging/ -n vorta-staging

      - name: ðŸ”” Configure Alerts
        run: |
          # Update alerting rules for staging
          kubectl apply -f infrastructure/kubernetes/alerting/staging-alerts.yaml -n vorta-staging

  # Notification
  notify-staging-deployment:
    name: ðŸ“¢ Notify Staging Status
    runs-on: ubuntu-latest
    needs:
      [pre-deployment, deploy-to-staging, staging-smoke-tests, staging-load-test, setup-monitoring]
    if: always() && needs.pre-deployment.outputs.should_deploy == 'true'

    steps:
      - name: ðŸ“¢ Success Notification
        if: success()
        run: |
          echo "ðŸŽ‰ Staging deployment completed successfully!"
          echo "ðŸ“¦ Version: ${{ needs.pre-deployment.outputs.version }}"
          echo "ðŸŒ Staging URL: ${{ needs.deploy-to-staging.outputs.deployment_url }}"
          echo "âœ… All tests passed"

      - name: ðŸ“¢ Failure Notification
        if: failure()
        run: |
          echo "âŒ Staging deployment encountered issues!"
          echo "ðŸ” Check the logs above for details"
          echo "â†©ï¸ Consider rollback if deployment is unstable"
          exit 1

  # Cleanup old deployments
  cleanup:
    name: ðŸ§¹ Cleanup Old Deployments
    runs-on: ubuntu-latest
    needs: [staging-smoke-tests]
    if: success()

    steps:
      - name: âš™ï¸ Setup Kubectl
        uses: azure/setup-kubectl@v3

      - name: ðŸ”‘ Configure Kubernetes Context
        run: |
          mkdir -p ~/.kube
          echo "${{ secrets.KUBE_CONFIG_STAGING || '# No kube config available' }}" | base64 -d > ~/.kube/config

      - name: ðŸ§¹ Remove Old ReplicaSets
        run: |
          # Keep only the 3 most recent ReplicaSets
          kubectl get replicasets -n vorta-staging \
            --sort-by=.metadata.creationTimestamp \
            -o json | \
            jq -r '.items[0:-3][]?.metadata.name' | \
            xargs -r kubectl delete replicaset -n vorta-staging

      - name: ðŸ§¹ Cleanup Old Container Images
        run: |
          # This would typically be done by a separate cleanup job
          echo "ðŸ§¹ Container cleanup scheduled for separate job"
